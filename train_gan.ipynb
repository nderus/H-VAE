{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conditional Variational autoencoder (VAE) - Toy datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir breast-histopathology\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip \"/content/drive/MyDrive/datasets/IDC_regular_ps50_idx5.zip\" -d \"/content\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv IDC_regular_ps50_idx5 breast-histopathology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/\n",
    "%rm -rf H-VAE\n",
    "!git clone https://github.com/nderus/H-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd H-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (0.12.17)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (5.9.1)\n",
      "Requirement already satisfied: PyYAML in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (1.5.12)\n",
      "Requirement already satisfied: setuptools in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (60.9.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: certifi in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from sentry-sdk>=1.0.0->wandb) (2022.6.15)\n",
      "Requirement already satisfied: urllib3>=1.10.0 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from sentry-sdk>=1.0.0->wandb) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/PERSONALE/nicolas.derus2/miniconda3/envs/dis_vae/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb\n",
    "import wandb\n",
    "from datasets import data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from tensorflow.keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: this should be passed as arguments\n",
    "dataset_name = 'histo'\n",
    "model_name = 'GAN'\n",
    "input_noise_dim=512\n",
    "epoch_count=50\n",
    "batch_size=100\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(lst, condition):\n",
    "    return np.array([i for i, elem in enumerate(lst) if condition(elem)])\n",
    "    \n",
    "def plot_2d_data_categorical(data_2d, y, titles=None, figsize = (7, 7), category_count=10):\n",
    "  fig, axs = plt.subplots(category_count, len(data_2d), figsize = figsize)\n",
    "  colors = np.array(['#7FFFD4', '#458B74', '#0000CD', '#EE3B3B', '#7AC5CD', '#66CD00',\n",
    "         '#EE7621', '#3D59AB', '#CD950C', '#483D8B'])\n",
    "  for i in range(len(data_2d)):\n",
    "      for k in range(category_count):\n",
    "\n",
    "        index = find_indices(y[i], lambda e: e == k)\n",
    "\n",
    "        data_2d_k = data_2d[i][index, ]\n",
    "        y_k = y[i][index]\n",
    "\n",
    "        if (titles != None):\n",
    "          axs[k,i].set_title(\"{} - Class: {}\".format(titles[i], k))\n",
    "\n",
    "        scatter = axs[k, i].scatter(data_2d_k[:, 0], data_2d_k[:, 1],\n",
    "                                s=1, c=colors[k], cmap=plt.cm.Paired)\n",
    "        axs[k, i].legend(*scatter.legend_elements())\n",
    "        axs[k, i].set_xlim([-3, 3])\n",
    "        axs[k, i].set_ylim([-3, 3])\n",
    "        wandb.log({\"Embdedding_classes\": wandb.Image(plt)})\n",
    "        \n",
    "def plot_2d_data(data_2d, y, titles=None, figsize = (7, 7)):\n",
    "  _, axs = plt.subplots(1, len(data_2d), figsize = figsize)\n",
    "\n",
    "  for i in range(len(data_2d)):\n",
    "    \n",
    "    if (titles != None):\n",
    "      axs[i].set_title(titles[i])\n",
    "    scatter=axs[i].scatter(data_2d[i][:, 0], data_2d[i][:, 1],\n",
    "                            s=1, c=y[i], cmap=plt.cm.Paired)\n",
    "    axs[i].legend(*scatter.legend_elements())\n",
    "    wandb.log({\"Embdedding\": wandb.Image(plt)})\n",
    "\n",
    "def plot_history(history,metric=None):\n",
    "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "  epoch_count=len(history.history['loss'])\n",
    "\n",
    "  line1,=ax1.plot(range(1,epoch_count+1),history.history['loss'],\n",
    "                  label='train_loss',color='orange')\n",
    "  ax1.plot(range(1,epoch_count+1),history.history['val_loss'],\n",
    "                  label='val_loss',color = line1.get_color(), linestyle = '--')\n",
    "  ax1.set_xlim([1,epoch_count])\n",
    "  ax1.set_ylim([0, max(max(history.history['loss']),\n",
    "              max(history.history['val_loss']))])\n",
    "  ax1.set_ylabel('loss',color = line1.get_color())\n",
    "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
    "  ax1.set_xlabel('Epochs')\n",
    "  _=ax1.legend(loc='lower left')\n",
    "\n",
    "  if (metric!=None):\n",
    "    ax2 = ax1.twinx()\n",
    "    line2,=ax2.plot(range(1,epoch_count+1),history.history[metric],\n",
    "                    label='train_'+metric)\n",
    "    ax2.plot(range(1,epoch_count+1),history.history['val_'+metric],\n",
    "                    label='val_'+metric,color = line2.get_color(),\n",
    "                    linestyle = '--')\n",
    "    ax2.set_ylim([0, max(max(history.history[metric]),\n",
    "                max(history.history['val_'+metric]))])\n",
    "    ax2.set_ylabel(metric,color=line2.get_color())\n",
    "    ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
    "    _=ax2.legend(loc='upper right')\n",
    "\n",
    "def plot_generated_images(generated_images, nrows, ncols,no_space_between_plots=False, figsize=(20, 20), epoch=None):\n",
    "  _, axs = plt.subplots(nrows, ncols,figsize=figsize,squeeze=False)\n",
    "\n",
    "  for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "      axs[i,j].axis('off')\n",
    "      axs[i,j].imshow((generated_images[i][j]* 255).astype(np.uint8))\n",
    "  \n",
    "\n",
    "  if no_space_between_plots:\n",
    "    plt.subplots_adjust(wspace=0,hspace=0)\n",
    "  wandb.log({\"Generations\": wandb.Image(plt, caption=\"Epoch:{}\".format( epoch)) }) #\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Val_Plot(loss, val_loss, reconstruction_loss, val_reconstruction_loss, kl_loss, val_kl_loss):\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,4, figsize= (16,4))\n",
    "    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n",
    "\n",
    "    ax1.plot(range(1, len(loss) + 1), loss)\n",
    "    ax1.plot(range(1, len(val_loss) + 1), val_loss)\n",
    "    ax1.set_title('History of Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(['training', 'validation'])\n",
    "\n",
    "    ax2.plot(range(1, len(reconstruction_loss) + 1), reconstruction_loss)\n",
    "    ax2.plot(range(1, len(val_reconstruction_loss) + 1), val_reconstruction_loss)\n",
    "    ax2.set_title('History of reconstruction_loss')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('reconstruction_loss')\n",
    "    ax2.legend(['training', 'validation'])\n",
    "    \n",
    "    ax3.plot(range(1, len(kl_loss) + 1), kl_loss)\n",
    "    ax3.plot(range(1, len(val_kl_loss) + 1), val_kl_loss)\n",
    "    ax3.set_title(' History of kl_loss')\n",
    "    ax3.set_xlabel(' Epochs ')\n",
    "    ax3.set_ylabel('kl_loss')\n",
    "    ax3.legend(['training', 'validation'])\n",
    "    wandb.log({\"Training\": wandb.Image(plt)})\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gan_losses(d_losses,g_losses):\n",
    "  fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "  epoch_count=len(d_losses)\n",
    "\n",
    "  line1,=ax1.plot(range(1,epoch_count+1),d_losses,label='discriminator_loss',color='orange')\n",
    "  ax1.set_ylim([0, max(d_losses)])\n",
    "  ax1.tick_params(axis='y', labelcolor=line1.get_color())\n",
    "  _=ax1.legend(loc='lower left')\n",
    "\n",
    "  ax2 = ax1.twinx()\n",
    "  line2,=ax2.plot(range(1,epoch_count+1),g_losses,label='generator_loss')\n",
    "  ax2.set_xlim([1,epoch_count])\n",
    "  ax2.set_ylim([0, max(g_losses)])\n",
    "  ax2.set_xlabel('Epochs')\n",
    "  ax2.tick_params(axis='y', labelcolor=line2.get_color())\n",
    "  _=ax2.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_random_input(batch_size,noise_dim,*_):\n",
    "  return np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
    "\n",
    "def get_gan_fake_batch(generator,batch_size,generator_input):\n",
    "  batch_x = generator.predict(generator_input)\n",
    "  batch_y=np.zeros(batch_size)\n",
    "\n",
    "  return batch_x,batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_cgan_batches(real_batch_x,fake_batch_x):\n",
    "  batch_input = np.concatenate((real_batch_x[0], fake_batch_x[0]))\n",
    "  batch_condition_info =np.concatenate((real_batch_x[1], fake_batch_x[1]))\n",
    "\n",
    "  return [batch_input,batch_condition_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(list, n):\n",
    "    for i in range(0, len(list), n):\n",
    "        yield list[i:i + n]\n",
    "\n",
    "def get_random_batch_indices(data_count,batch_size):\n",
    "    list_indices=list(range(0,data_count))\n",
    "    random.shuffle(list_indices)\n",
    "    return list(chunks(list_indices, batch_size))\n",
    "\n",
    "def get_cgan_real_batch(dataset,batch_indices,label):\n",
    "  dataset_input=dataset[0]\n",
    "  dataset_condition_info=dataset[1]\n",
    "  batch_x =[dataset_input[batch_indices],dataset_condition_info[batch_indices]]\n",
    "  batch_y=np.full(len(batch_indices),label)\n",
    "\n",
    "  return batch_x,batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cgan_random_input(batch_size,noise_dim,condition_count):\n",
    "  noise=np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
    "  condition_info= to_categorical(np.random.randint(0, condition_count, size=batch_size),condition_count)\n",
    "\n",
    "  return [noise,condition_info]\n",
    "\n",
    "def get_cgan_fake_batch(generator,batch_size,generator_input):\n",
    "  batch_x = [generator.predict(generator_input),generator_input[1]]\n",
    "  batch_y=np.zeros(batch_size)\n",
    "\n",
    "  return batch_x,batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan,generator,discriminator,train_x,train_data_count,input_noise_dim,epoch_count, batch_size,\n",
    "              get_random_input_func,get_real_batch_func,get_fake_batch_func,concatenate_batches_func,condition_count=-1,\n",
    "              use_one_sided_labels=False,plt_frq=None,plt_example_count=10,example_shape=(64,64,3)):\n",
    "    iteration_count = int(train_data_count / batch_size)\n",
    "    \n",
    "    print('Epochs: ', epoch_count)\n",
    "    print('Batch size: ', batch_size)\n",
    "    print('Iterations: ', iteration_count)\n",
    "    print('')\n",
    "    \n",
    "    #Plot generated images\n",
    "    if plt_frq!=None:\n",
    "      print('Before training:')\n",
    "      noise_to_plot = get_random_input_func(plt_example_count, input_noise_dim,condition_count)\n",
    "      generated_output = generator.predict(noise_to_plot)\n",
    "      generated_images = generated_output.reshape(plt_example_count, example_shape[0], example_shape[1],  example_shape[2])\n",
    "      plot_generated_images([generated_images],1,plt_example_count,figsize=(15, 5), epoch=epoch_count)\n",
    "          \n",
    "    d_epoch_losses=[]\n",
    "    g_epoch_losses=[]\n",
    "\n",
    "    \n",
    "    for e in range(1, epoch_count+1):\n",
    "        start_time = time.time()\n",
    "        avg_d_loss=0\n",
    "        avg_g_loss=0\n",
    "\n",
    "        # Training indices are shuffled and grouped into batches\n",
    "        batch_indices=get_random_batch_indices(train_data_count,batch_size)\n",
    "\n",
    "        for i in range(iteration_count):\n",
    "            current_batch_size=len(batch_indices[i])\n",
    "\n",
    "            # 1. create a batch with real images from the training set\n",
    "            real_batch_x,real_batch_y=get_real_batch_func(train_x,batch_indices[i],0.9 if use_one_sided_labels else 1)\n",
    "                        \n",
    "            # 2. create noise vectors for the generator and generate the images from the noise\n",
    "            generator_input=get_random_input_func(current_batch_size, input_noise_dim,condition_count)\n",
    "            fake_batch_x,fake_batch_y=get_fake_batch_func(generator,current_batch_size,generator_input)\n",
    "\n",
    "            # 3. concatenate real and fake batches into a single batch\n",
    "            discriminator_batch_x = concatenate_batches_func(real_batch_x, fake_batch_x)\n",
    "            discriminator_batch_y= np.concatenate((real_batch_y, fake_batch_y))\n",
    "\n",
    "            # 4. train discriminator\n",
    "            d_loss = discriminator.train_on_batch(discriminator_batch_x, discriminator_batch_y)\n",
    "            \n",
    "            # 5. create noise vectors for the generator\n",
    "            gan_batch_x = get_random_input_func(current_batch_size, input_noise_dim,condition_count)\n",
    "            gan_batch_y = np.ones(current_batch_size)    #Flipped labels\n",
    "\n",
    "            # 6. train generator\n",
    "            g_loss = gan.train_on_batch(gan_batch_x, gan_batch_y)\n",
    "\n",
    "            # 7. avg losses\n",
    "            avg_d_loss+=d_loss*current_batch_size\n",
    "            avg_g_loss+=g_loss*current_batch_size\n",
    "            \n",
    "        avg_d_loss/=train_data_count\n",
    "        avg_g_loss/=train_data_count\n",
    "\n",
    "        d_epoch_losses.append(avg_d_loss)\n",
    "        g_epoch_losses.append(avg_g_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        print('Epoch: {0} exec_time={1:.1f}s d_loss={2:.3f} g_loss={3:.3f}'.format(e,end_time - start_time,avg_d_loss,avg_g_loss))\n",
    "\n",
    "        # Update the plots\n",
    "        if plt_frq!=None and e%plt_frq == 0:\n",
    "            generated_output = generator.predict(noise_to_plot)\n",
    "            generated_images = generated_output.reshape(plt_example_count, example_shape[0], example_shape[1],  example_shape[2])\n",
    "            plot_generated_images([generated_images],1,plt_example_count,figsize=(15, 5), epoch=e)\n",
    "    \n",
    "    return d_epoch_losses,g_epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data import and manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/PERSONALE/nicolas.derus2/H-VAE/train_gan.ipynb Cella 24\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/H-VAE/train_gan.ipynb#ch0000014vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m#TO DO: move datasets in the repo and change root_folder\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/H-VAE/train_gan.ipynb#ch0000014vscode-remote?line=2'>3</a>\u001b[0m train_x, test_x, val_x, train_y, test_y, val_y, train_y_one_hot, test_y_one_hot, val_y_one_hot, input_shape, category_count, labels \u001b[39m=\u001b[39m data_loader(name\u001b[39m=\u001b[39;49mdataset_name,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B137.204.48.211/home/PERSONALE/nicolas.derus2/H-VAE/train_gan.ipynb#ch0000014vscode-remote?line=3'>4</a>\u001b[0m                                                                                                                                      root_folder\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/content/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/H-VAE/datasets.py:102\u001b[0m, in \u001b[0;36mdata_loader\u001b[0;34m(name, root_folder)\u001b[0m\n\u001b[1;32m     99\u001b[0m         class1\u001b[39m.\u001b[39mappend(filename)\n\u001b[1;32m    101\u001b[0m random\u001b[39m.\u001b[39mseed(\u001b[39m11\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m sampled_class0 \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49msample(class0, \u001b[39m50000\u001b[39;49m) \u001b[39m# TO DO: use whole dataset\u001b[39;00m\n\u001b[1;32m    103\u001b[0m random\u001b[39m.\u001b[39mseed(\u001b[39m11\u001b[39m)\n\u001b[1;32m    104\u001b[0m sampled_class1 \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(class1, \u001b[39m50000\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dis_vae/lib/python3.9/random.py:449\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    447\u001b[0m randbelow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_randbelow\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m k \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSample larger than population or is negative\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    450\u001b[0m result \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m k\n\u001b[1;32m    451\u001b[0m setsize \u001b[39m=\u001b[39m \u001b[39m21\u001b[39m        \u001b[39m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "#TO DO: move datasets in the repo and change root_folder\n",
    "\n",
    "train_x, test_x, val_x, train_y, test_y, val_y, train_y_one_hot, test_y_one_hot, val_y_one_hot, input_shape, category_count, labels = data_loader(name=dataset_name,\n",
    "                                                                                                                                     root_folder='/content/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GAN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "#wandb.init(project=\"my-test-project\", entity=\"nrderus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login a32cf68901332ce5f39557dc9f6a8d328f07098b --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 5\n",
    "\n",
    "\n",
    "wandb.init(project=\"GAN\", entity=\"nrderus\",\n",
    "  config = {\n",
    "  \"dataset\": dataset_name,\n",
    "  \"model\": \"CVAE\",\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epoch_count,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"patience\": patience,\n",
    "  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cdcgan(input_noise_dim,condition_dim):\n",
    "  input_noise=layers.Input(shape=input_noise_dim, name='input_noise')\n",
    "  input_condition=layers.Input(shape=condition_dim, name='input_condition')\n",
    "\n",
    "  input_noise_reshaped=layers.Reshape((1,1,512))(input_noise)\n",
    "  input_condition_reshaped=layers.Reshape((1,1,2))(input_condition)\n",
    "\n",
    "  #Generator\n",
    "  generator_input = layers.Concatenate(name='generator_input')([input_noise_reshaped, input_condition_reshaped])\n",
    "\n",
    "  prev_layer=layers.Conv2DTranspose(512,8,strides=2,padding='valid')(generator_input)\n",
    "  prev_layer=layers.BatchNormalization()(prev_layer)\n",
    "  prev_layer=layers.LeakyReLU(alpha=0.2)(prev_layer)\n",
    "\n",
    "\n",
    "  prev_layer=layers.Conv2DTranspose(128,3,strides=2,padding='same')(prev_layer)\n",
    "  prev_layer=layers.BatchNormalization()(prev_layer)\n",
    "  prev_layer=layers.LeakyReLU(alpha=0.2)(prev_layer)\n",
    "\n",
    "  prev_layer=layers.Conv2DTranspose(64,3,strides=2,padding='same')(prev_layer)\n",
    "  prev_layer=layers.BatchNormalization()(prev_layer)\n",
    "  prev_layer=layers.LeakyReLU(alpha=0.2)(prev_layer)\n",
    "\n",
    "\n",
    "  prev_layer=layers.Conv2DTranspose(64,3,strides=2,padding='same')(prev_layer)\n",
    "  prev_layer=layers.BatchNormalization()(prev_layer)\n",
    "  prev_layer=layers.LeakyReLU(alpha=0.2)(prev_layer)\n",
    "\n",
    "  generator_output=layers.Conv2DTranspose(3,3,strides=1,padding='same',activation='tanh',name='generator_output')(prev_layer)\n",
    "\n",
    "  generator = keras.Model([input_noise,input_condition], generator_output, name='generator')\n",
    "\n",
    "  #Discriminator\n",
    "  \n",
    "\n",
    "  discriminator_input_sample = layers.Input(shape=(64,64,3), name='discriminator_input_sample')\n",
    "\n",
    "  input_condition_dense=layers.Dense(64*64)(input_condition)\n",
    "  discriminator_input_condition=layers.Reshape((64,64,1))(input_condition_dense)\n",
    "\n",
    "  discriminator_input = layers.Concatenate(name='discriminator_input')([discriminator_input_sample, discriminator_input_condition])\n",
    "  prev_layer=layers.Conv2D(64,3,strides=2,padding='same')(discriminator_input)\n",
    "  prev_layer=layers.BatchNormalization()(prev_layer)\n",
    "  prev_layer=layers.LeakyReLU(alpha=0.2)(prev_layer)\n",
    "\n",
    "  prev_layer=layers.Conv2D(128,5,strides=2,padding='same')(prev_layer)\n",
    "  prev_layer=layers.BatchNormalization()(prev_layer)\n",
    "  prev_layer=layers.LeakyReLU(alpha=0.2)(prev_layer)\n",
    "\n",
    "  prev_layer=layers.Conv2D(256,5,strides=2,padding='same')(prev_layer)\n",
    "  prev_layer=layers.BatchNormalization()(prev_layer)\n",
    "  prev_layer=layers.LeakyReLU(alpha=0.2)(prev_layer)\n",
    "\n",
    "  prev_layer=layers.Conv2D(512,3,strides=2,padding='same')(prev_layer)\n",
    "  prev_layer=layers.BatchNormalization()(prev_layer)\n",
    "  prev_layer=layers.LeakyReLU(alpha=0.2)(prev_layer)\n",
    "\n",
    "  prev_layer=layers.Conv2D(1, 4,strides=1,padding='valid',activation='sigmoid')(prev_layer)\n",
    "\n",
    "  discriminator_output=layers.Reshape((1,),name='discriminator_output')(prev_layer)\n",
    "\n",
    "  discriminator = keras.Model([discriminator_input_sample,input_condition], discriminator_output, name='discriminator')\n",
    "\n",
    "\n",
    "\n",
    "  #cDCGAN\n",
    "  cdcgan = keras.Model(generator.input, discriminator([generator.output,input_condition]),name='cdcgan')\n",
    "  \n",
    "  return cdcgan,generator,discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cdcgan,cdcgan_generator,cdcgan_discriminator=build_cdcgan(input_noise_dim,category_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdcgan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "\n",
    "cdcgan_discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "cdcgan_discriminator.trainable = False\n",
    "cdcgan.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "d_epoch_losses,g_epoch_losses=train_gan(cdcgan,\n",
    "                                        cdcgan_generator,\n",
    "                                        cdcgan_discriminator,\n",
    "                                        [val_x,val_y_one_hot],\n",
    "                                        val_x.shape[0],\n",
    "                                        input_noise_dim,\n",
    "                                        epoch_count,\n",
    "                                        batch_size,\n",
    "                                        get_cgan_random_input,\n",
    "                                        get_cgan_real_batch,\n",
    "                                        get_cgan_fake_batch,\n",
    "                                        concatenate_cgan_batches,\n",
    "                                        condition_count=category_count,\n",
    "                                        use_one_sided_labels=True,\n",
    "                                        plt_frq=1,\n",
    "                                        plt_example_count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gan_losses(d_epoch_losses,g_epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_label=0\n",
    "\n",
    "noise = np.random.normal(0, 1, size=(1, input_noise_dim))\n",
    "digit_label_one_hot=to_categorical(digit_label, category_count).reshape(1,-1)\n",
    "\n",
    "generated_x = cdcgan_generator.predict([noise,digit_label_one_hot])\n",
    "digit = generated_x[0].reshape(input_shape)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(digit, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 # number of images per digit category\n",
    "\n",
    "generated_images=[]\n",
    "for digit_label in range(category_count):\n",
    "  noise = np.random.normal(0, 1, size=(n, input_noise_dim))\n",
    "  digit_label_one_hot=to_categorical(np.full(n,digit_label), category_count)\n",
    "  generated_x = cdcgan_generator.predict([noise,digit_label_one_hot])\n",
    "  generated_images.append([g.reshape(input_shape) for g in generated_x])\n",
    "\n",
    "plot_generated_images(generated_images,category_count,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish(exit_code=0, quiet = True) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0284a593613b942586af6b8f0d4ee916e356aed836174e2f823c929bc6bc05cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dis_vae')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
